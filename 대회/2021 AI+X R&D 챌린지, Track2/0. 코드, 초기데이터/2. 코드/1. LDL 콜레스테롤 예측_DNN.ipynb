{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2017,
     "status": "ok",
     "timestamp": 1640755877223,
     "user": {
      "displayName": "김동영",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10909136525301516030"
     },
     "user_tz": -540
    },
    "id": "eSWE6jjWTavG",
    "outputId": "7b8a3d62-7cec-4f71-87f8-e1f1f1d7413b"
   },
   "outputs": [],
   "source": [
    "'''Read train/validate data from separated CSV files.'''\n",
    "# Read data using Pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_set = pd.read_csv(\"../1. 초기데이터/Data_Set_LDL-C_trainset.csv\")\n",
    "val_set = pd.read_csv(\"../1. 초기데이터/Data_Set_LDL-C_valset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1640756375164,
     "user": {
      "displayName": "김동영",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10909136525301516030"
     },
     "user_tz": -540
    },
    "id": "wbnXdkOrr3mL",
    "outputId": "90b584ab-2dd5-4730-ba7d-fb7c4c7acac4"
   },
   "outputs": [],
   "source": [
    "'''Divide train data and test data. (X_data is train data Y_data is test data.)'''\n",
    "import numpy as np\n",
    "\n",
    "# Extract Train Data\n",
    "# x_train is data for train DNN.\n",
    "x_train = train_set.iloc[:,[3,4,6]]\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "# y_train is data for evaluate output of DNN.\n",
    "y_train = train_set.iloc[:,[5]]\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Extract Test Data\n",
    "# x_test is data for train DNN.\n",
    "x_val = val_set.iloc[:,[3,4,6]]\n",
    "x_val = np.array(x_val)\n",
    "\n",
    "# y_test is data for evaluate output of DNN.\n",
    "y_val = val_set.iloc[:,[5]]\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNugIs0H5rVd"
   },
   "outputs": [],
   "source": [
    "'''Data Normalization'''\n",
    "\n",
    "mean = x_train.mean(axis=0)\n",
    "std = x_train.std(axis=0)\n",
    "\n",
    "x_train -= mean\n",
    "x_train /= std\n",
    "\n",
    "x_val -= mean\n",
    "x_val /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alHD5koJ5v5_"
   },
   "outputs": [],
   "source": [
    "'''Configuration of model'''\n",
    "from tensorflow.keras import models,layers\n",
    "from keras import backend as K\n",
    "\n",
    "# User defined loss function\n",
    "def squared_error(y_true, y_pred):\n",
    "    return (float(y_true) - float(y_pred)) ** 2\n",
    "\n",
    "# User defined metrics\n",
    "# Reference: https://stackify.dev/168133-kerasregressor-coefficient-of-determination-r2-score\n",
    "def coeff_determination(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square( y_true-y_pred )) \n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) ) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "# Build DNN model.\n",
    "def build_model(): \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(30, activation='relu',\n",
    "                           input_dim=3))\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='relu'))\n",
    "    model.compile(optimizer='adam', loss=squared_error, metrics=[coeff_determination])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''K-fold cross validation'''\n",
    "# Reference: https://morningcoding.tistory.com/entry/Keras%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%B3%B4%EC%8A%A4%ED%84%B4-%EC%A3%BC%ED%83%9D%EA%B0%80%EA%B2%A9-%EC%98%88%EC%B8%A1-K-Fold\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Hyperparameter of this train\n",
    "k = 5\n",
    "num_epochs = 100\n",
    "batch = 80\n",
    "\n",
    "num_val_samples = len(x_train) // k\n",
    "\n",
    "all_r2_histories = []\n",
    "all_scores = []\n",
    "start_time = time.time() \n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    print('Processing Fold #', i)\n",
    "    val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]  \n",
    "    val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    \n",
    "    partial_train_data = np.concatenate(  \n",
    "        [x_train[:i * num_val_samples],\n",
    "         x_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [y_train[:i * num_val_samples],\n",
    "         y_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    model = build_model()  \n",
    "    history = model.fit(partial_train_data, partial_train_targets, \n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=batch, verbose=1)\n",
    "    r2_history = history.history['coeff_determination']\n",
    "    all_r2_histories.append(r2_history)\n",
    "end_time = time.time()\n",
    "print(\"Running Time: \",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_r2_history = [\n",
    "    np.mean([x[i] for x in all_r2_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQPMekKx58HA"
   },
   "outputs": [],
   "source": [
    "\"\"\"Visualization of train result\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_r2_history) + 1), average_r2_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uovvte8H6Blx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Visualization convert to smooth curve\"\"\"\n",
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "smooth_r2_history = smooth_curve(average_r2_history[10:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_r2_history) + 1), smooth_r2_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('R2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwsLAEuq6Qjr"
   },
   "outputs": [],
   "source": [
    "'''Train model using validate set data'''\n",
    "num_epochs = 100\n",
    "batch = 80\n",
    "\n",
    "model = build_model()  \n",
    "model.fit(x_val, y_val,  \n",
    "          epochs=num_epochs, batch_size=batch, verbose=1)\n",
    "val_mse_score, val_mae_score = model.evaluate(x_val, y_val)\n",
    "print(val_mse_score)\n",
    "print(val_mae_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Assume predict value'''\n",
    "x_hat = x_val\n",
    "y_hat = model.predict(x_hat, use_multiprocessing=True)\n",
    "print('Estimate : ',y_hat)\n",
    "print(x_hat.shape)\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import math\n",
    "\n",
    "# Calculate Determination Coefficient\n",
    "r2 = r2_score(y_val, y_hat)\n",
    "print('R2 : ', r2)\n",
    "# Calculate Correlation Coefficient\n",
    "r = math.sqrt(r2)\n",
    "print('R : ', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save predict_data(y_hat) to Data_Set_LDL-C-DNN.csv'''\n",
    "df_CD = pd.DataFrame(y_hat,columns=['DNN'])\n",
    "df_CD.to_csv('../1. 초기데이터/Data_Set_LDL-C-DNN.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN3D4bTDH9r9HOrf6CGwc+h",
   "mount_file_id": "1eVQV2jduBFmvmI4nS27RGLjtWKY4n8Zz",
   "name": "콜레스테롤 테스트.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "ffa16020a8c6a060eed1f6e0d04d47b12b670243870a2b7848585651411f27fc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
