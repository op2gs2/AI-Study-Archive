{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "- 토큰: 텍스트를 처리하는 단위. 단어나 문장, 문자로 지정가능\n",
    "- CountVectorizer(): 텍스트를 입력 받아, 텍스트 토큰의 개수로 구성된 벡터를 출력\n",
    "- HashingVectorizer(): CountVectorizer()에 비해 더 빠르고 확장성이 좋음.\n",
    "    - 다만, 문서 코퍼스에 등장하는 단어 수를 세는 것으로는 misleading 할 수 있음.\n",
    "    - 불용어(like, a, ...)라는 중요하지 않은 단어들의 발생 빈도수가 높으며, 이러한 단어는 정보가 없기 때문.\n",
    "- 가중치: 불용어를 제거하고자 부여하는 값. 주로 TF-IDF를 이용\n",
    "- TF-IDF (Term-Frequency, Inverse-Document-Frequency): 단어 빈도와 역문서 빈도\n",
    "    - 단어가 나타나는 횟수를 감안하되, 해당 단어가 들어가 있는 문서의 수로 단어의 빈도를 상쇄함.\n",
    "- 사이버보안에는 텍스트 데이터가 많으므로, 이런 데이터를 다룰 줄  알아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 938273)\t0.10023429482560929\n",
      "  (0, 871172)\t-0.33044470291777067\n",
      "  (0, 755834)\t-0.2806123960092745\n",
      "  (0, 556974)\t-0.2171490773135763\n",
      "  (0, 548264)\t-0.09851435603064428\n",
      "  (0, 531189)\t-0.2566310842337745\n",
      "  (0, 522961)\t-0.3119912982467716\n",
      "  (0, 514190)\t-0.2527659565181208\n",
      "  (0, 501800)\t-0.33044470291777067\n",
      "  (0, 499727)\t-0.18952297847436425\n",
      "  (0, 488876)\t0.13502094828386488\n",
      "  (0, 377854)\t0.22710724511856722\n",
      "  (0, 334594)\t-0.25581186158424035\n",
      "  (0, 256577)\t0.20949022238574433\n",
      "  (0, 197273)\t-0.30119674850360456\n",
      "  (0, 114899)\t0.09713499033205285\n",
      "  (0, 28523)\t-0.3060506288368513\n",
      "  (1, 960098)\t0.09780838928665199\n",
      "  (1, 955748)\t-0.2747271490090429\n",
      "  (1, 952302)\t0.26070217969901804\n",
      "  (1, 938273)\t0.12095603891963835\n",
      "  (1, 937092)\t-0.2947114257264502\n",
      "  (1, 927866)\t0.21727726371674563\n",
      "  (1, 820768)\t-0.11065660403137358\n",
      "  (1, 772066)\t-0.14344517367198276\n",
      "  :\t:\n",
      "  (180828, 329790)\t0.06808618130417012\n",
      "  (180828, 312887)\t-0.08249409552977467\n",
      "  (180828, 209871)\t0.17685927011939476\n",
      "  (180828, 193711)\t-0.14127016157231428\n",
      "  (180828, 181881)\t-0.11885031537539834\n",
      "  (180828, 180525)\t-0.06925490785130799\n",
      "  (180828, 156500)\t-0.20787461071537122\n",
      "  (180828, 148568)\t0.1963433059906426\n",
      "  (180828, 82508)\t-0.1289257787752738\n",
      "  (180828, 79994)\t0.23121076025389292\n",
      "  (180828, 78098)\t-0.18205107240120946\n",
      "  (180828, 47738)\t0.23121076025389292\n",
      "  (180828, 46353)\t0.1045181919567425\n",
      "  (180828, 45900)\t-0.09537730182105167\n",
      "  (180828, 45419)\t-0.11189579574426382\n",
      "  (180828, 11712)\t-0.16947494737589616\n",
      "  (180829, 1026910)\t0.4082112914772047\n",
      "  (180829, 975831)\t-0.18401193506169794\n",
      "  (180829, 936283)\t0.2472007199039777\n",
      "  (180829, 856299)\t-0.15436175878438183\n",
      "  (180829, 473183)\t-0.41092004816695277\n",
      "  (180829, 464504)\t0.2928849862993687\n",
      "  (180829, 251872)\t-0.4714000763194845\n",
      "  (180829, 189128)\t0.44418614795477124\n",
      "  (180829, 45900)\t-0.20102520636796686\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 데이터를 불러온다.\n",
    "with open(\"./datasets/anonops_short.txt\",encoding=\"utf8\") as f:\n",
    "    anonops_chat_logs = f.readlines()\n",
    "\n",
    "# HashingVectorizer를 사용해 텍스트에서 단어의 수를 세고, TF-IDF를 사용해 가중값을 부여한다.\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "my_vector = HashingVectorizer(input=\"content\", ngram_range=(1,2)) # one-gram / bi-gram 단어의 갯수를 계산한다.\n",
    "X_train_counts = my_vector.fit_transform(anonops_chat_logs,)\n",
    "tf_transformer = TfidfTransformer(use_idf=True,).fit(X_train_counts) # HashingVectorizer에 적절한 가중값을 부여함.\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "# 최종 결과는 각 행이 텍스트 중의 하나를 나타내는 희소행렬이다.\n",
    "print(X_train_tf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}