{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "directories_with_lables = [(\"DA Logs Benign\", 0), (\"DA Logs Malware\", 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# JSON 로그를 구문 분석하는 함수\n",
    "def get_API_class_method_type_from_log(log):\n",
    "    # 행위 로그에서 나온 API 호출을 구문 분석한다.\n",
    "    API_data_sequence = []\n",
    "    with open(log) as log_file:\n",
    "        json_log = json.load(log_file)\n",
    "        api_calls_array = \"[\" + json_log[\"api_calls\"] + \"]\"\n",
    "        # 클래스, 메소드, API 호출 유형을 선택함\n",
    "        api_calls = json.load(api_calls_array)\n",
    "            for api_call in api_calls:\n",
    "                data = api_call['class'] + \":\" + api_call[\"method\"] + \":\" + api_call[\"type\"]\n",
    "                API_data_sequence.append(data)\n",
    "        return API_data_sequence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 로그를 Corpus로 읽고 레이블을 수집함\n",
    "data_corpus = []\n",
    "labels = []\n",
    "for directory, label in directories_with_lables:\n",
    "    logs = os.listdir(directory)\n",
    "    for log_path in logs:\n",
    "        file_path = directory + \"/\" + log_path\n",
    "        try:\n",
    "            data_corpus.append(get_API_class_method_type_from_log(file_path))\n",
    "            labels.append(label)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 말뭉치 안에 있는 데이터 확인하기\n",
    "print(data_corpus[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 훈련 데이터와 테스트 데이터를 분할한다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "corpus_train, corpus_test, y_train, y_test = train_test_split(\n",
    "    data_corpus, labels, test_size=0.2, random_state=11\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 현재 데이터 형식을 수정하는, N-Gram 추출 함수를 불러옴\n",
    "import collections\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "\n",
    "def read_file(file_path):\n",
    "    # 이진 파일의 문자열을 읽는다.\n",
    "    with open(file_path, \"rb\") as binary_file:\n",
    "        data = binary_file.read()\n",
    "    return data\n",
    "\n",
    "def text_to_Ngrams(text, n):\n",
    "    # 텍스트에서 N-그램 리스트를 만든다.\n",
    "    Ngrams = ngrams(text, n)\n",
    "    return list(Ngrams)\n",
    "\n",
    "def get_Ngram_counts(text, N):\n",
    "    # 텍스트에서 N-그램의 빈도수 갯수를 얻는다.\n",
    "    Ngrams = text_to_Ngrams(text, N)\n",
    "    return collections.Counter(Ngrams)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# N-그램을 수집한다.\n",
    "N = 4\n",
    "total_Ngram_count = collections.Counter([])\n",
    "for file in corpus_train:\n",
    "    total_Ngram_count += get_Ngram_counts(file, N)\n",
    "    \n",
    "# 가장 빈도수가 높은 N-그램(K1)을 3000개로 좁힌다.\n",
    "K1 = 3000\n",
    "K1_most_frequent_Ngrams = total_Ngram_count.most_common(K1)\n",
    "K1_most_frequent_Ngrams_list = [x[0] for x in K1_most_frequent_Ngrams]\n",
    "K1_most_frequent_Ngrams_list[:7]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 샘플들을 N-그램 카운트 벡터로 특성화하는 함수를 만든다.\n",
    "def featurize_sample(file, Ngram_list):\n",
    "    # 샘플의 특성 벡터를 만든다. 특성은 우리가 선택한 N-그램 K1개의 갯수다.\n",
    "    K1 = len(Ngram_list)\n",
    "    feature_vector = K1 * [0]\n",
    "    fileNgrams = get_Ngram_counts(file, N)\n",
    "    for i in range(K1):\n",
    "        feature_vector[i] = fileNgrams[Ngram_list[i]]\n",
    "    return feature_vector\n",
    "\n",
    "# 위 함수를 훈련 샘플과 테스트 샘플을 특성화하는데 적용한다.\n",
    "X_train = []\n",
    "for sample in corpus_train:\n",
    "    X_train.append(featurize_sample(sample, K1_most_frequent_Ngrams_list))\n",
    "X_train = np.asarray(X_train)\n",
    "\n",
    "X_test = []\n",
    "for sample in corpus_train:\n",
    "    X_test.append(featurize_sample(sample, K1_most_frequent_Ngrams_list))\n",
    "X_test = np.asarray(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 상호정보를 사용해 가장 빈도수가 높은 N-그램(K1=3000개)을 가장 많은 정보를 가진 N-그램(K2=500개)로 좁힌다. 그런 다음 파이프라인을 설정해 XGBoost 분류기를 실행한다.\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "K2 = 500\n",
    "mi_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"mutual_information\", SelectKBest(mutual_info_classif, k=K2)),\n",
    "        (\"xgb\", XGBClassifier())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 파이프라인을 훈련하고, 정확도를 평가함.\n",
    "mi_pipeline.fit(X_train, y_train)\n",
    "print(\"훈련 정확도: \",mi_pipeline.score(X_train,y_train))\n",
    "print(\"테스트 정확도: \",mi_pipeline.score(X_test,y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}