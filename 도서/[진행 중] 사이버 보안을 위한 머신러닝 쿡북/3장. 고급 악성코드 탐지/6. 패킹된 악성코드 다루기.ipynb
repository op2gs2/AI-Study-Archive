{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''패킹된 샘플 데이터셋 만들기'''\n",
    "import os\n",
    "\n",
    "files_path = \"./Benign PE Samples UPX/\"\n",
    "files = os.listdir(files_path)\n",
    "files_paths = [files_path + x for x in files]\n",
    "\n",
    "# 디렉터리 B에 있는 각 파일에 대해 upx를 실행한다.\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "cmd = 'upx.exe'\n",
    "for path in files_paths:\n",
    "    cmd2 = cmd + \"\\\"\" + path + \"\\\"\"\n",
    "    res = Popen(cmd2, stdout=PIPE).communicate()\n",
    "    print(res)\n",
    "\n",
    "# 패킹 시 에러가 발생할 때마다 원래의 샘플을 삭제한다.\n",
    "if \"error\" in str(res[0]):\n",
    "    print(path)\n",
    "    os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''패커 분류기 구축'''\n",
    "import os\n",
    "\n",
    "# 사용된 패커에 해당하는 레이블과 함께, 분석해야 할 파일의 이름을 읽는다.\n",
    "directories_with_labels = [\n",
    "    (\"Benign PE Samples\", 0),\n",
    "    (\"Benign PE Samples UPX\", 1),\n",
    "    (\"Benign PE Samples Amber\", 2),\n",
    "]\n",
    "list_of_samples = []\n",
    "labels = []\n",
    "for dataset_path, label in directories_with_labels:\n",
    "    samples = [f for f in os.listdir(dataset_path)]\n",
    "    for file in samples:\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        list_of_samples.append(file_path)\n",
    "        labels.append(label)\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 분할한다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "samples_train, samples_test, labels_train, labels_test = train_test_split(\n",
    "    list_of_samples, labels, test_size=0.3, stratify=labels, random_state=11\n",
    ")\n",
    "\n",
    "# N-그램 추출에 사용할 함수\n",
    "import collections\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "\n",
    "def read_file(file_path):\n",
    "    # 이진 파일의 문자열을 읽는다.\n",
    "    with open(file_path, 'rb') as binary_file:\n",
    "        data = binary_file.read()\n",
    "    return data\n",
    "\n",
    "def byte_sequence_to_Ngrams(byte_sequence, N):\n",
    "    # 바이트 문자열에서 N-그램 리스트를 만든다.\n",
    "    Ngrams = ngrams(byte_sequence, N)\n",
    "    return list(Ngrams)\n",
    "\n",
    "def extract_Ngrams_counts(file, N):\n",
    "    # 이진 파일을 읽고, 이진 문자열에서 N-그램의 갯수를 출력한다.\n",
    "    filebyte_sequence = read_file(file)\n",
    "    file_Ngrams = byte_sequence_to_Ngrams(filebyte_sequence, N)\n",
    "    return collections.Counter(file_Ngrams)\n",
    "\n",
    "def featurize_sample(sample, K1_most_frequent_Ngrams_list):\n",
    "    # 샘플의 특성 벡터를 만든다. 특성은 우리가 선택한 N-그램의 K1개의 갯수이다.\n",
    "    K1 = len(K1_most_frequent_Ngrams_list)\n",
    "    feature_vector = K1 * [0]\n",
    "    file_Ngrams = extract_Ngrams_counts(sample, N)\n",
    "    for i in range(K1):\n",
    "        feature_vector[i] = file_Ngrams[K1_most_frequent_Ngrams_list[i]]\n",
    "    return feature_vector"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 데이터에 대해 특성으로 사용하려는 N-그램을 선택한다.\n",
    "N = 2\n",
    "total_Ngram_count = collections.Counter([])\n",
    "for file in samples_train:\n",
    "    total_Ngram_count += extract_Ngrams_counts(file, N)\n",
    "\n",
    "K1 = 100\n",
    "K1_most_common_Ngrams = total_Ngram_count.most_common(K1)\n",
    "K1_most_common_Ngrams = [x[0] for x in K1_most_common_Ngrams]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 훈련데이터를 특성화한다.\n",
    "Ngram_features_list_train = []\n",
    "y_train = []\n",
    "for i in range(len(samples_train)):\n",
    "    file = samples_train[i]\n",
    "    Ngram_features = featurize_sample(file, K1_most_common_Ngrams)\n",
    "    Ngram_features_list_train.append(Ngram_features)\n",
    "    y_train.append(labels_train[i])\n",
    "X_train = Ngram_features_list_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 훈련 데이터에 대해 랜덤 포레스트 모델을 훈련한다.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 테스트 데이터를 특성화한다.\n",
    "Ngram_features_list_test = []\n",
    "y_test = []\n",
    "for i in range(len(samples_test)):\n",
    "    file = samples_test[i]\n",
    "    Ngram_features = featurize_sample(file, K1_most_common_Ngrams)\n",
    "    Ngram_features_list_train.append(Ngram_features)\n",
    "    y_train.append(labels_test[i])\n",
    "X_test = Ngram_features_list_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 훈련한 분류기를 사용해 테스트 데이터를 예측하고, 혼동행렬을 사용해 성능을 평가한다.\n",
    "y_pred = clf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}