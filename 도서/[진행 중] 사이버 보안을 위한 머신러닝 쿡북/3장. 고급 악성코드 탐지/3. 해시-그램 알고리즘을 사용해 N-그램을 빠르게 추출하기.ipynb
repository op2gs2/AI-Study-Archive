{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_13856/1640213015.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"C:\\Users\\pando\\AppData\\Local\\Temp/ipykernel_13856/1640213015.py\"\u001B[1;36m, line \u001B[1;32m26\u001B[0m\n\u001B[1;33m    def hash_file_Ngrams_into_directory\u001B[0m\n\u001B[1;37m                                       ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from nltk import ngrams\n",
    "import hashlib\n",
    "\n",
    "directories = [\"Benign PE Samples\", \"Malicious PE Samples\"]\n",
    "N = 2\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, \"rb\") as binary_file:\n",
    "        data = binary_file.read()\n",
    "    return data\n",
    "\n",
    "# 바이트 시퀸스에서 N-그램 리스트를 만든다.\n",
    "def byte_sequence_to_Ngrams(byte_sequence, N):\n",
    "    return ngrams(byte_sequence, N)\n",
    "\n",
    "# 입력값의 해시값을 계산한다.\n",
    "def hash_input(inp):\n",
    "    return int(hashlib.md5(inp).hexdigest(), 16)\n",
    "\n",
    "# N-그램을 바이트로 변환한다.\n",
    "def make_ngram_hashable(Ngram):\n",
    "    return bytes(Ngram)\n",
    "\n",
    "# N-그램의 해시값을 계산한 다음, 해시값에 대한 딕셔너리의 카운트를 증가시킨다.\n",
    "def hash_file_Ngrams_into_directory(file_Ngrams, T):\n",
    "    for Ngram in file_Ngrams:\n",
    "        hashable_Ngram = make_ngram_hashable(Ngram)\n",
    "        hashed_and_reduced = hash_input(hashable_Ngram) % B # 축소모듈 B는 딕셔너리에 B 키값만 입력하도록 함.\n",
    "        T[hashed_and_reduced] = T.get(hashed_and_reduced, 0) + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "B = 65521 # 2^16보다 작으면서 가장 큰 소수를 B로 지정\n",
    "T = {}\n",
    "\n",
    "# 해시값을 계산한 N-그램의 갯수를 센다.\n",
    "for dataset_path in directories:\n",
    "    samples = [ f for f in listdir(dataset_path)]\n",
    "    for file in samples:\n",
    "        file_path = dataset_path + \"/\" + file\n",
    "        file_byte_sequence = read_file(file_path)\n",
    "        file_Ngrams = byte_sequence_to_Ngrams(file_byte_sequence, N)\n",
    "        hash_file_Ngrams_into_directory(file_Ngrams, T)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "# 가장 빈도수가 높은 K1=1000개를 선택\n",
    "K1 = 1000\n",
    "K1_most_common_Ngrams_Using_Hash_Grams = heapq.nlargest(K1, T)\n",
    "\n",
    "# 가장 빈도수가 높은 N-그램의 해시값을 선택해 특성 집합으로 구성함\n",
    "def featurize_sample(file,K1_most_common_Ngrams_Using_Hash_Grams):\n",
    "    # 샘플의 특성 벡터를 만든다. 특성은 우리가 선택한 K1개의 N-그램의 갯수다.\n",
    "    K1 = len(K1_most_common_Ngrams_Using_Hash_Grams)\n",
    "    fv = K1 * [0]\n",
    "    file_byte_sequence = read_file(file_path)\n",
    "    file_Ngrams = byte_sequence_to_Ngrams(file_byte_sequence, N)\n",
    "    for Ngram in file_Ngrams:\n",
    "        hashable_Ngram = make_ngram_hashable(Ngram)\n",
    "        hashed_and_reduced = hash_input(hashable_Ngram) % B\n",
    "        if hashed_and_reduced in K1_most_common_Ngrams_Using_Hash_Grams:\n",
    "            index = K1_most_common_Ngrams_Using_Hash_Grams.index(hashed_and_reduced)\n",
    "            fv[index] += 1\n",
    "    return fv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 데이터 셋을 특성화\n",
    "X = []\n",
    "for dataset_path in directories:\n",
    "    samples = [f for f in listdir(dataset_path)]\n",
    "    for file in samples:\n",
    "        file_path = dataset_path + \"/\" + file\n",
    "        X.append(featurize_sample(file_path, K1_most_common_Ngrams_Using_Hash_Grams))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}