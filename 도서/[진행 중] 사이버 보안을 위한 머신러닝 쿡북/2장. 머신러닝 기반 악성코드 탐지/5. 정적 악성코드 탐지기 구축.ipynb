{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: 'Benign PE Samples'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_1756/2116219232.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mdataset_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdirectories_with_labels\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m     \u001B[0msamples\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mf\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mlistdir\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset_path\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0msample\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msamples\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[0mfile_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataset_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: 'Benign PE Samples'"
     ]
    }
   ],
   "source": [
    "'''정적 악성코드 탐지기 구축'''\n",
    "# 샘플을 열거한 다음, 레이블을 지정한다.\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "directories_with_labels = [(\"Benign PE Samples\", 0), (\"Malicious PE Samples\", 0)]\n",
    "list_of_samples = []\n",
    "labels = []\n",
    "for dataset_path, label in directories_with_labels:\n",
    "    samples = [f for f in listdir(dataset_path)]\n",
    "    for sample in samples:\n",
    "        file_path = os.path.join(dataset_path, sample)\n",
    "        list_of_samples.append(file_path)\n",
    "        labels.append(label)\n",
    "\n",
    "# 충화 훈련-테스트 분할을 수행한다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "samples_train, samples_test, labels_train, labels_test = train_test_split(list_of_samples, labels, test_size=0.3, stratify=labels, random_state=11)\n",
    "\n",
    "# 특성을 얻고자 이전 레시피의 편의함수를 사용한다.\n",
    "import collections\n",
    "from nltk import ngrams\n",
    "import numpy as np\n",
    "import pefile\n",
    "\n",
    "# 파일을 바이트로 읽기 위한 편의함수를 작성한다.\n",
    "def read_file(file_path):\n",
    "    # 이진 파일의 문자열을 읽는다.\n",
    "    with open(file_path, \"rb\") as binary_file:\n",
    "        data = binary_file.read()\n",
    "    return data\n",
    "\n",
    "# 바이트 문자열을 가져와 N-그램을 얻는 편의 함수를 정의한다.\n",
    "def byte_sequence_to_Ngrams(byte_sequence, N):\n",
    "    # 바이트 문자열에서 N-그램 리스트를 만든다.\n",
    "    Ngrams = ngrams(byte_sequence, N)\n",
    "    return list(Ngrams)\n",
    "\n",
    "# 파일을 읽고 N-그램의 빈도수를 계산하는 함수를 작성한다.\n",
    "def binary_file_to_Ngram_counts(file, N):\n",
    "    # 이진 파일을 읽고, 이진 문자열에서 N-그램의 개수를 출력한다.\n",
    "    filebyte_sequence = read_file(file)\n",
    "    file_Ngrams = byte_sequence_to_Ngrams(filebyte_sequence, 4)\n",
    "    return collections.Counter(file_Ngrams)\n",
    "\n",
    "def get_NGram_features_from_sample(sample, K1_most_frequent_Ngrams_list):\n",
    "    # 샘플에서 특성 벡터를 만든다. 특성은 우리가 선택한 N-그램 K1개의 빈도수다.\n",
    "    K1 = len(K1_most_frequent_Ngrams_list)\n",
    "    feature_vector = K1 * [0]\n",
    "    file_Ngrams = binary_file_to_Ngram_counts(sample, N)\n",
    "    for i in range(K1):\n",
    "        feature_vector[i] = file_Ngrams[K1_most_frequent_Ngrams_list[i]]\n",
    "    return feature_vector\n",
    "\n",
    "def perprocess_imports(list_of_DLLs):\n",
    "    # PE 파일에서 들여온 것의 이름을 소문자로 정규화 한다.\n",
    "    temp = [X.decode().split(\".\")[0].lower() for x in list_of_DLLs]\n",
    "    return \" \".join(temp)\n",
    "\n",
    "def get_imports(pe):\n",
    "    # PE 파일에서 Import 항목의 리스트를 만든다.\n",
    "    list_of_imports = []\n",
    "    for entry in pe.DIRECTORY_ENTRY_IMPORT:\n",
    "        list_of_imports.append(entry.dll)\n",
    "    return preprocess_imports(list_of_imports)\n",
    "\n",
    "def get_section_names(pe):\n",
    "    # PE 파일에서 섹션의 이름 리스트를 만든다.\n",
    "    list_of_section_names = []\n",
    "    for sec in pe.sections:\n",
    "        normalized_name = sec.Name.decode().replace(\"\\x00\", \"\").lower()\n",
    "        list_of_section_names.append(normalized_name)\n",
    "    return \"\".join(list_of_section_names)\n",
    "\n",
    "# 가장 빈도수가 높은 2-그램 100개를 특성으로 선택한다.\n",
    "N = 2\n",
    "Ngram_count_all = collections.Counter([])\n",
    "for sample in samples_train:\n",
    "    Ngram_count_all += binary_file_to_Ngram_counts(sample, N)\n",
    "K1 = 100\n",
    "K1_most_frequent_Ngrams = Ngram_count_all.most_common(K1)\n",
    "K1_most_frequent_Ngrams_list = [x[0] for x in K1_most_frequent_Ngrams]\n",
    "\n",
    "# 훈련 과정에서 각 샘플의 N-그램 갯수와 섹션 이름, Imports, 섹션의 갯수를 추출하고, 파싱할 수 없는 PE헤더 샘플은 건너뛴다.\n",
    "imports_corpus_train = []\n",
    "num_sections_train = []\n",
    "section_names_train = []\n",
    "Ngram_features_list_train = []\n",
    "Y_train = []\n",
    "for i in range(len(samples_train)):\n",
    "    sample = samples_train[i]\n",
    "    try:\n",
    "        Ngram_features = get_NGram_features_from_sample(sample, K1_most_frequent_Ngrams_list)\n",
    "        pe = pefile.PE(sample)\n",
    "        imports = get_imports(pe)\n",
    "        n_sections = len(pe.sections)\n",
    "        sec_names = get_section_names(pe)\n",
    "        imports_corpus_train.append(imports)\n",
    "        num_sections_train.append(sec_names)\n",
    "        Ngram_features_list_train.append(Ngram_features)\n",
    "        Y_train.append(labels_train[i])\n",
    "    except Exception as e:\n",
    "        print(sample + \":\")\n",
    "        print(e)\n",
    "\n",
    "# TF-IDF와 HashVectorizer를 사용해 2개의 Text Feature, Imports, 섹션 이름을 숫자 형식으로 변환한다.\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "imports_featurizer = Pipeline(\n",
    "    [\n",
    "        (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1,2))),\n",
    "        (\"tfidf\", TfidfTransformer(use_idf=True, )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "section_names_featurizer = Pipeline(\n",
    "    [\n",
    "        (\"vect\", HashingVectorizer(input=\"content\", ngram_range=(1,2))),\n",
    "        (\"tfidf\", TfidfTransformer(use_idf=True, )),\n",
    "    ]\n",
    ")\n",
    "\n",
    "imports_corpus_train_transformed = imports_featurizer.fit_transform(imports_corpus_train)\n",
    "section_names_train_transformed = section_names_featurizer.fit_transform(section_names_train)\n",
    "\n",
    "# 벡터화된 특성을 하나의 배열로 만든다\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "X_train = hstack(\n",
    "    [\n",
    "        Ngram_features_list_train,\n",
    "        imports_corpus_train_transformed,\n",
    "        section_names_train_transformed,\n",
    "        csr_matrix(num_sections_train).transpose(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 훈련 데이터에 대해 랜덤 포레스트 분류기를 훈련시키고 그 점수를 출력한다.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "# 텍스트 데이터셋의 특성을 수집한 다음, 훈련 데이터셋에 대해 수행한 것을 반복한다.\n",
    "imports_corpus_test = []\n",
    "num_sections_test = []\n",
    "section_names_test = []\n",
    "Ngram_features_list_test = []\n",
    "Y_test = []\n",
    "for i in range(len(samples_test)):\n",
    "    file = samples_test[i]\n",
    "    try:\n",
    "        Ngram_features = get_NGram_features_from_sample(sample, K1_most_frequent_Ngrams_list)\n",
    "        pe = pefile.PE(file)\n",
    "        imports = get_imports(pe)\n",
    "        n_sections = len(pe.sections)\n",
    "        sec_names = get_section_names(pe)\n",
    "        imports_corpus_test.append(n_sections)\n",
    "        section_names_test.append(n_sections)\n",
    "        section_names_test.append(sec_names)\n",
    "        Ngram_features_list_test.append(Ngram_features)\n",
    "        Y_test.append(labels_test[i])\n",
    "    except Exception as e:\n",
    "        print(sample + \":\")\n",
    "        print(e)\n",
    "\n",
    "# 앞서 훈련한 변환기를 적용해 텍스트 특성을 벡터로 만든 다음, 남아 있는 테스트셋으로 분류기를 테스트 한다.\n",
    "imports_corpus_test_transformed = imports_featurizer.transform(imports_corpus_test)\n",
    "section_names_test_transformed = section_names_featurizer.transform(section_names_test)\n",
    "X_test = hstack(\n",
    "    [\n",
    "        Ngram_features_list_test,\n",
    "        imports_corpus_test_transformed,\n",
    "        section_names_test_transformed,\n",
    "        csr_matrix(num_sections_test).transpose(),\n",
    "    ]\n",
    ")\n",
    "print(f\"테스트셋에 대한 분류기 점수: {clf.score(X_test, Y_test)*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 120, 1: 11})\n",
      "모델 예측 점수: 86.66666666666667\n",
      "Counter({0: 114, 1: 17})\n",
      "모델 예측 점수: 99.13793103448276\n",
      "Counter({0: 114, 1: 17})\n",
      "모델 예측 점수: 99.13793103448276\n",
      "Counter({0: 104, 1: 27})\n",
      "모델 예측 점수: 94.82758620689656\n",
      "Counter({0: 114, 1: 17})\n",
      "모델 예측 점수: 99.13793103448276\n"
     ]
    }
   ],
   "source": [
    "'''계급 불균형 해결'''\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import collections\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 읽고, 의사결정트리와 성능 점수를 계산하는데 사용할 라이브러리를 가져옴\n",
    "X_train = scipy.sparse.load_npz(\"./datasets/training_data.npz\")\n",
    "Y_train = np.load(\"./datasets/training_labels.npy\")\n",
    "X_test = scipy.sparse.load_npz(\"./datasets/test_data.npz\")\n",
    "Y_test = np.load(\"./datasets/test_labels.npy\")\n",
    "\n",
    "# 의사결정 트리 분류기를 훈련하고 테스트한다.\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(X_train, Y_train)\n",
    "dt_pred = dt.predict(X_test)\n",
    "print(collections.Counter(dt_pred))\n",
    "print(\"모델 예측 점수: {}\".format(balanced_accuracy_score(Y_test, dt_pred)*100))\n",
    "\n",
    "# 가중값을 적용한다. 분류기의 계급 가중값을 Balanced로 설정한 다음, 새로운 분류기를 훈련하고 테스트 한다.\n",
    "dt_weighted = tree.DecisionTreeClassifier(class_weight=\"balanced\")\n",
    "dt_weighted.fit(X_train, Y_train)\n",
    "dt_weighted_pred = dt_weighted.predict(X_test)\n",
    "print(collections.Counter(dt_weighted_pred))\n",
    "print(\"모델 예측 점수: {}\".format(balanced_accuracy_score(Y_test, dt_weighted_pred)*100))\n",
    "\n",
    "# 계급의 원소가 적은 계급에 대해 샘플링을 더 많이 한다. 계급 0과 계급 1에서 모든 테스트 샘플을 추출한다.\n",
    "from sklearn.utils import resample\n",
    "\n",
    "X_train_np = X_train.toarray()\n",
    "class_0_indices = [i for i,x in enumerate(Y_train == 0) if x]\n",
    "class_1_indices = [i for i,x in enumerate(Y_train == 1) if x]\n",
    "size_class_0 = sum(Y_train == 0)\n",
    "X_train_class_0 = X_train_np[class_0_indices, :]\n",
    "Y_train_class_0 = [0] * size_class_0\n",
    "X_train_class_1 = X_train_np[class_1_indices, :]\n",
    "\n",
    "# 계급 1과 계급 0의 표본의 갯수가 같아질 때까지, 계급 1의 원소를 복원 샘플링한다.\n",
    "X_train_class_1_resampled = resample(\n",
    "    X_train_class_1,  replace=True, n_samples=size_class_0\n",
    ")\n",
    "Y_train_class_1_resampled = [1] * size_class_0\n",
    "\n",
    "# 새로 업샘플링한 샘플을 단일 훈련 데이터셋으로 만든다.\n",
    "X_train_resampled = np.concatenate([X_train_class_0, X_train_class_1_resampled])\n",
    "Y_train_resampled = Y_train_class_0 + Y_train_class_1_resampled\n",
    "\n",
    "# 업샘플링한 훈련 데이터셋으로 랜덤 포레스트 분류기로 훈련하고 테스트한다.\n",
    "from scipy import sparse\n",
    "\n",
    "X_train_resampled = sparse.csr_matrix(X_train_resampled)\n",
    "dt_resampled = tree.DecisionTreeClassifier()\n",
    "dt_resampled.fit(X_train_resampled, Y_train_resampled)\n",
    "dt_resampled_pred = dt_resampled.predict(X_test)\n",
    "print(collections.Counter(dt_resampled_pred))\n",
    "print(\"모델 예측 점수: {}\".format(balanced_accuracy_score(Y_test, dt_resampled_pred)*100))\n",
    "\n",
    "# 계급의 원소가 많은 계급(Major Class)에 대해 샘플링을 더 적게 한다. 이전 업샘플링과 비슷한 단계를 수행하지만, 이번에는 더 적은 계급의 수와 같아질 때까지 더 많은 계급에서 다운 샘플링한다.\n",
    "X_train_np = X_train.toarray()\n",
    "class_0_indices = [i for i,x in enumerate(Y_train == 0) if x]\n",
    "class_1_indices = [i for i,x in enumerate(Y_train == 1) if x]\n",
    "size_class_1 = sum(Y_train == 1)\n",
    "X_train_class_1 = X_train_np[class_1_indices, :]\n",
    "Y_train_class_1 = [1] * size_class_1\n",
    "X_train_class_0 = X_train_np[class_0_indices, :]\n",
    "X_train_class_0_downsampled = resample(\n",
    "    X_train_class_0, replace=False, n_samples=size_class_1\n",
    ")\n",
    "Y_train_class_0_downsampled = [0] * size_class_1\n",
    "\n",
    "# 새로 업샘플링한 샘플을 단일 훈련 데이터셋으로 만든다.\n",
    "X_train_downsampled = np.concatenate([X_train_class_1, X_train_class_0_downsampled])\n",
    "Y_train_downsampled = Y_train_class_1 + Y_train_class_0_downsampled\n",
    "\n",
    "# 다운 샘플링한 훈련 데이터셋으로 랜덤 포레스트 분류기를 훈련하고 테스트한다.\n",
    "X_train_downsampled = sparse.csr_matrix(X_train_downsampled)\n",
    "dt_downsampled = tree.DecisionTreeClassifier()\n",
    "dt_downsampled.fit(X_train_downsampled,Y_train_downsampled)\n",
    "dt_downsampled_pred = dt_downsampled.predict(X_test)\n",
    "print(collections.Counter(dt_downsampled_pred))\n",
    "print(\"모델 예측 점수: {}\".format(balanced_accuracy_score(Y_test, dt_downsampled_pred)*100))\n",
    "\n",
    "# 내부 균형 샘플러를 포함한 분류기를 사용한다. 훈련 추정기에 앞서 데이터의 부분집합을 재샘플링하는 불균형-학습 패키지 분류기를 사용한다.\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "balanced_clf = BalancedBaggingClassifier(\n",
    "    base_estimator=tree.DecisionTreeClassifier(),\n",
    "    sampling_strategy=\"auto\",\n",
    "    replacement=True\n",
    ")\n",
    "balanced_clf.fit(X_train, Y_train)\n",
    "balanced_clf_pred = balanced_clf.predict(X_test)\n",
    "print(collections.Counter(balanced_clf_pred))\n",
    "print(\"모델 예측 점수: {}\".format(balanced_accuracy_score(Y_test, balanced_clf_pred)*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\AI-Study-Archive\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:08:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "확률은 다음과 같다: \n",
      "\n",
      "[[9.9696845e-01 3.0315337e-03]\n",
      " [9.9934214e-01 6.5786147e-04]\n",
      " [9.9936205e-01 6.3797331e-04]\n",
      " [9.9046874e-01 9.5312512e-03]\n",
      " [9.1151476e-01 8.8485263e-02]]\n",
      "한계점 적합 중: \n",
      "999번째 시도 한계점: 0.999, 오탐률: 0.4636, 정탐률: 1.0000\n",
      "998번째 시도 한계점: 0.998, 오탐률: 0.3273, 정탐률: 1.0000\n",
      "997번째 시도 한계점: 0.997, 오탐률: 0.2636, 정탐률: 1.0000\n",
      "996번째 시도 한계점: 0.996, 오탐률: 0.1955, 정탐률: 1.0000\n",
      "995번째 시도 한계점: 0.995, 오탐률: 0.1682, 정탐률: 1.0000\n",
      "994번째 시도 한계점: 0.994, 오탐률: 0.1636, 정탐률: 1.0000\n",
      "993번째 시도 한계점: 0.993, 오탐률: 0.1500, 정탐률: 1.0000\n",
      "992번째 시도 한계점: 0.992, 오탐률: 0.1409, 정탐률: 1.0000\n",
      "991번째 시도 한계점: 0.991, 오탐률: 0.1273, 정탐률: 1.0000\n",
      "990번째 시도 한계점: 0.990, 오탐률: 0.1227, 정탐률: 1.0000\n",
      "989번째 시도 한계점: 0.989, 오탐률: 0.1136, 정탐률: 1.0000\n",
      "988번째 시도 한계점: 0.988, 오탐률: 0.1091, 정탐률: 1.0000\n",
      "987번째 시도 한계점: 0.987, 오탐률: 0.1091, 정탐률: 1.0000\n",
      "986번째 시도 한계점: 0.986, 오탐률: 0.1045, 정탐률: 1.0000\n",
      "985번째 시도 한계점: 0.985, 오탐률: 0.1045, 정탐률: 1.0000\n",
      "984번째 시도 한계점: 0.984, 오탐률: 0.1000, 정탐률: 1.0000\n",
      "983번째 시도 한계점: 0.983, 오탐률: 0.1000, 정탐률: 1.0000\n",
      "982번째 시도 한계점: 0.982, 오탐률: 0.1000, 정탐률: 1.0000\n",
      "981번째 시도 한계점: 0.981, 오탐률: 0.1000, 정탐률: 1.0000\n",
      "980번째 시도 한계점: 0.980, 오탐률: 0.1000, 정탐률: 1.0000\n",
      "979번째 시도 한계점: 0.979, 오탐률: 0.0955, 정탐률: 1.0000\n",
      "978번째 시도 한계점: 0.978, 오탐률: 0.0955, 정탐률: 1.0000\n",
      "977번째 시도 한계점: 0.977, 오탐률: 0.0955, 정탐률: 1.0000\n",
      "976번째 시도 한계점: 0.976, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "975번째 시도 한계점: 0.975, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "974번째 시도 한계점: 0.974, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "973번째 시도 한계점: 0.973, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "972번째 시도 한계점: 0.972, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "971번째 시도 한계점: 0.971, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "970번째 시도 한계점: 0.970, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "969번째 시도 한계점: 0.969, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "968번째 시도 한계점: 0.968, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "967번째 시도 한계점: 0.967, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "966번째 시도 한계점: 0.966, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "965번째 시도 한계점: 0.965, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "964번째 시도 한계점: 0.964, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "963번째 시도 한계점: 0.963, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "962번째 시도 한계점: 0.962, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "961번째 시도 한계점: 0.961, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "960번째 시도 한계점: 0.960, 오탐률: 0.0773, 정탐률: 1.0000\n",
      "959번째 시도 한계점: 0.959, 오탐률: 0.0682, 정탐률: 1.0000\n",
      "958번째 시도 한계점: 0.958, 오탐률: 0.0682, 정탐률: 1.0000\n",
      "957번째 시도 한계점: 0.957, 오탐률: 0.0682, 정탐률: 1.0000\n",
      "956번째 시도 한계점: 0.956, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "955번째 시도 한계점: 0.955, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "954번째 시도 한계점: 0.954, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "953번째 시도 한계점: 0.953, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "952번째 시도 한계점: 0.952, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "951번째 시도 한계점: 0.951, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "950번째 시도 한계점: 0.950, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "949번째 시도 한계점: 0.949, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "948번째 시도 한계점: 0.948, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "947번째 시도 한계점: 0.947, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "946번째 시도 한계점: 0.946, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "945번째 시도 한계점: 0.945, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "944번째 시도 한계점: 0.944, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "943번째 시도 한계점: 0.943, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "942번째 시도 한계점: 0.942, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "941번째 시도 한계점: 0.941, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "940번째 시도 한계점: 0.940, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "939번째 시도 한계점: 0.939, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "938번째 시도 한계점: 0.938, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "937번째 시도 한계점: 0.937, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "936번째 시도 한계점: 0.936, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "935번째 시도 한계점: 0.935, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "934번째 시도 한계점: 0.934, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "933번째 시도 한계점: 0.933, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "932번째 시도 한계점: 0.932, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "931번째 시도 한계점: 0.931, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "930번째 시도 한계점: 0.930, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "929번째 시도 한계점: 0.929, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "928번째 시도 한계점: 0.928, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "927번째 시도 한계점: 0.927, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "926번째 시도 한계점: 0.926, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "925번째 시도 한계점: 0.925, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "924번째 시도 한계점: 0.924, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "923번째 시도 한계점: 0.923, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "922번째 시도 한계점: 0.922, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "921번째 시도 한계점: 0.921, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "920번째 시도 한계점: 0.920, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "919번째 시도 한계점: 0.919, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "918번째 시도 한계점: 0.918, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "917번째 시도 한계점: 0.917, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "916번째 시도 한계점: 0.916, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "915번째 시도 한계점: 0.915, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "914번째 시도 한계점: 0.914, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "913번째 시도 한계점: 0.913, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "912번째 시도 한계점: 0.912, 오탐률: 0.0636, 정탐률: 1.0000\n",
      "911번째 시도 한계점: 0.911, 오탐률: 0.0591, 정탐률: 1.0000\n",
      "910번째 시도 한계점: 0.910, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "909번째 시도 한계점: 0.909, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "908번째 시도 한계점: 0.908, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "907번째 시도 한계점: 0.907, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "906번째 시도 한계점: 0.906, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "905번째 시도 한계점: 0.905, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "904번째 시도 한계점: 0.904, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "903번째 시도 한계점: 0.903, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "902번째 시도 한계점: 0.902, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "901번째 시도 한계점: 0.901, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "900번째 시도 한계점: 0.900, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "899번째 시도 한계점: 0.899, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "898번째 시도 한계점: 0.898, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "897번째 시도 한계점: 0.897, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "896번째 시도 한계점: 0.896, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "895번째 시도 한계점: 0.895, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "894번째 시도 한계점: 0.894, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "893번째 시도 한계점: 0.893, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "892번째 시도 한계점: 0.892, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "891번째 시도 한계점: 0.891, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "890번째 시도 한계점: 0.890, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "889번째 시도 한계점: 0.889, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "888번째 시도 한계점: 0.888, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "887번째 시도 한계점: 0.887, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "886번째 시도 한계점: 0.886, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "885번째 시도 한계점: 0.885, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "884번째 시도 한계점: 0.884, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "883번째 시도 한계점: 0.883, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "882번째 시도 한계점: 0.882, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "881번째 시도 한계점: 0.881, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "880번째 시도 한계점: 0.880, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "879번째 시도 한계점: 0.879, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "878번째 시도 한계점: 0.878, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "877번째 시도 한계점: 0.877, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "876번째 시도 한계점: 0.876, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "875번째 시도 한계점: 0.875, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "874번째 시도 한계점: 0.874, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "873번째 시도 한계점: 0.873, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "872번째 시도 한계점: 0.872, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "871번째 시도 한계점: 0.871, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "870번째 시도 한계점: 0.870, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "869번째 시도 한계점: 0.869, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "868번째 시도 한계점: 0.868, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "867번째 시도 한계점: 0.867, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "866번째 시도 한계점: 0.866, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "865번째 시도 한계점: 0.865, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "864번째 시도 한계점: 0.864, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "863번째 시도 한계점: 0.863, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "862번째 시도 한계점: 0.862, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "861번째 시도 한계점: 0.861, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "860번째 시도 한계점: 0.860, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "859번째 시도 한계점: 0.859, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "858번째 시도 한계점: 0.858, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "857번째 시도 한계점: 0.857, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "856번째 시도 한계점: 0.856, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "855번째 시도 한계점: 0.855, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "854번째 시도 한계점: 0.854, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "853번째 시도 한계점: 0.853, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "852번째 시도 한계점: 0.852, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "851번째 시도 한계점: 0.851, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "850번째 시도 한계점: 0.850, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "849번째 시도 한계점: 0.849, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "848번째 시도 한계점: 0.848, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "847번째 시도 한계점: 0.847, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "846번째 시도 한계점: 0.846, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "845번째 시도 한계점: 0.845, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "844번째 시도 한계점: 0.844, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "843번째 시도 한계점: 0.843, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "842번째 시도 한계점: 0.842, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "841번째 시도 한계점: 0.841, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "840번째 시도 한계점: 0.840, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "839번째 시도 한계점: 0.839, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "838번째 시도 한계점: 0.838, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "837번째 시도 한계점: 0.837, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "836번째 시도 한계점: 0.836, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "835번째 시도 한계점: 0.835, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "834번째 시도 한계점: 0.834, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "833번째 시도 한계점: 0.833, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "832번째 시도 한계점: 0.832, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "831번째 시도 한계점: 0.831, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "830번째 시도 한계점: 0.830, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "829번째 시도 한계점: 0.829, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "828번째 시도 한계점: 0.828, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "827번째 시도 한계점: 0.827, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "826번째 시도 한계점: 0.826, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "825번째 시도 한계점: 0.825, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "824번째 시도 한계점: 0.824, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "823번째 시도 한계점: 0.823, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "822번째 시도 한계점: 0.822, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "821번째 시도 한계점: 0.821, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "820번째 시도 한계점: 0.820, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "819번째 시도 한계점: 0.819, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "818번째 시도 한계점: 0.818, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "817번째 시도 한계점: 0.817, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "816번째 시도 한계점: 0.816, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "815번째 시도 한계점: 0.815, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "814번째 시도 한계점: 0.814, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "813번째 시도 한계점: 0.813, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "812번째 시도 한계점: 0.812, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "811번째 시도 한계점: 0.811, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "810번째 시도 한계점: 0.810, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "809번째 시도 한계점: 0.809, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "808번째 시도 한계점: 0.808, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "807번째 시도 한계점: 0.807, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "806번째 시도 한계점: 0.806, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "805번째 시도 한계점: 0.805, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "804번째 시도 한계점: 0.804, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "803번째 시도 한계점: 0.803, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "802번째 시도 한계점: 0.802, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "801번째 시도 한계점: 0.801, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "800번째 시도 한계점: 0.800, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "799번째 시도 한계점: 0.799, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "798번째 시도 한계점: 0.798, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "797번째 시도 한계점: 0.797, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "796번째 시도 한계점: 0.796, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "795번째 시도 한계점: 0.795, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "794번째 시도 한계점: 0.794, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "793번째 시도 한계점: 0.793, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "792번째 시도 한계점: 0.792, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "791번째 시도 한계점: 0.791, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "790번째 시도 한계점: 0.790, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "789번째 시도 한계점: 0.789, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "788번째 시도 한계점: 0.788, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "787번째 시도 한계점: 0.787, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "786번째 시도 한계점: 0.786, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "785번째 시도 한계점: 0.785, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "784번째 시도 한계점: 0.784, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "783번째 시도 한계점: 0.783, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "782번째 시도 한계점: 0.782, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "781번째 시도 한계점: 0.781, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "780번째 시도 한계점: 0.780, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "779번째 시도 한계점: 0.779, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "778번째 시도 한계점: 0.778, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "777번째 시도 한계점: 0.777, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "776번째 시도 한계점: 0.776, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "775번째 시도 한계점: 0.775, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "774번째 시도 한계점: 0.774, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "773번째 시도 한계점: 0.773, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "772번째 시도 한계점: 0.772, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "771번째 시도 한계점: 0.771, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "770번째 시도 한계점: 0.770, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "769번째 시도 한계점: 0.769, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "768번째 시도 한계점: 0.768, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "767번째 시도 한계점: 0.767, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "766번째 시도 한계점: 0.766, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "765번째 시도 한계점: 0.765, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "764번째 시도 한계점: 0.764, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "763번째 시도 한계점: 0.763, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "762번째 시도 한계점: 0.762, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "761번째 시도 한계점: 0.761, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "760번째 시도 한계점: 0.760, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "759번째 시도 한계점: 0.759, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "758번째 시도 한계점: 0.758, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "757번째 시도 한계점: 0.757, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "756번째 시도 한계점: 0.756, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "755번째 시도 한계점: 0.755, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "754번째 시도 한계점: 0.754, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "753번째 시도 한계점: 0.753, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "752번째 시도 한계점: 0.752, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "751번째 시도 한계점: 0.751, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "750번째 시도 한계점: 0.750, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "749번째 시도 한계점: 0.749, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "748번째 시도 한계점: 0.748, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "747번째 시도 한계점: 0.747, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "746번째 시도 한계점: 0.746, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "745번째 시도 한계점: 0.745, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "744번째 시도 한계점: 0.744, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "743번째 시도 한계점: 0.743, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "742번째 시도 한계점: 0.742, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "741번째 시도 한계점: 0.741, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "740번째 시도 한계점: 0.740, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "739번째 시도 한계점: 0.739, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "738번째 시도 한계점: 0.738, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "737번째 시도 한계점: 0.737, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "736번째 시도 한계점: 0.736, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "735번째 시도 한계점: 0.735, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "734번째 시도 한계점: 0.734, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "733번째 시도 한계점: 0.733, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "732번째 시도 한계점: 0.732, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "731번째 시도 한계점: 0.731, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "730번째 시도 한계점: 0.730, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "729번째 시도 한계점: 0.729, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "728번째 시도 한계점: 0.728, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "727번째 시도 한계점: 0.727, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "726번째 시도 한계점: 0.726, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "725번째 시도 한계점: 0.725, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "724번째 시도 한계점: 0.724, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "723번째 시도 한계점: 0.723, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "722번째 시도 한계점: 0.722, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "721번째 시도 한계점: 0.721, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "720번째 시도 한계점: 0.720, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "719번째 시도 한계점: 0.719, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "718번째 시도 한계점: 0.718, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "717번째 시도 한계점: 0.717, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "716번째 시도 한계점: 0.716, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "715번째 시도 한계점: 0.715, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "714번째 시도 한계점: 0.714, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "713번째 시도 한계점: 0.713, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "712번째 시도 한계점: 0.712, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "711번째 시도 한계점: 0.711, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "710번째 시도 한계점: 0.710, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "709번째 시도 한계점: 0.709, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "708번째 시도 한계점: 0.708, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "707번째 시도 한계점: 0.707, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "706번째 시도 한계점: 0.706, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "705번째 시도 한계점: 0.705, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "704번째 시도 한계점: 0.704, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "703번째 시도 한계점: 0.703, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "702번째 시도 한계점: 0.702, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "701번째 시도 한계점: 0.701, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "700번째 시도 한계점: 0.700, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "699번째 시도 한계점: 0.699, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "698번째 시도 한계점: 0.698, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "697번째 시도 한계점: 0.697, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "696번째 시도 한계점: 0.696, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "695번째 시도 한계점: 0.695, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "694번째 시도 한계점: 0.694, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "693번째 시도 한계점: 0.693, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "692번째 시도 한계점: 0.692, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "691번째 시도 한계점: 0.691, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "690번째 시도 한계점: 0.690, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "689번째 시도 한계점: 0.689, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "688번째 시도 한계점: 0.688, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "687번째 시도 한계점: 0.687, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "686번째 시도 한계점: 0.686, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "685번째 시도 한계점: 0.685, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "684번째 시도 한계점: 0.684, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "683번째 시도 한계점: 0.683, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "682번째 시도 한계점: 0.682, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "681번째 시도 한계점: 0.681, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "680번째 시도 한계점: 0.680, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "679번째 시도 한계점: 0.679, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "678번째 시도 한계점: 0.678, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "677번째 시도 한계점: 0.677, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "676번째 시도 한계점: 0.676, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "675번째 시도 한계점: 0.675, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "674번째 시도 한계점: 0.674, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "673번째 시도 한계점: 0.673, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "672번째 시도 한계점: 0.672, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "671번째 시도 한계점: 0.671, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "670번째 시도 한계점: 0.670, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "669번째 시도 한계점: 0.669, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "668번째 시도 한계점: 0.668, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "667번째 시도 한계점: 0.667, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "666번째 시도 한계점: 0.666, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "665번째 시도 한계점: 0.665, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "664번째 시도 한계점: 0.664, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "663번째 시도 한계점: 0.663, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "662번째 시도 한계점: 0.662, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "661번째 시도 한계점: 0.661, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "660번째 시도 한계점: 0.660, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "659번째 시도 한계점: 0.659, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "658번째 시도 한계점: 0.658, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "657번째 시도 한계점: 0.657, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "656번째 시도 한계점: 0.656, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "655번째 시도 한계점: 0.655, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "654번째 시도 한계점: 0.654, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "653번째 시도 한계점: 0.653, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "652번째 시도 한계점: 0.652, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "651번째 시도 한계점: 0.651, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "650번째 시도 한계점: 0.650, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "649번째 시도 한계점: 0.649, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "648번째 시도 한계점: 0.648, 오탐률: 0.0545, 정탐률: 1.0000\n",
      "647번째 시도 한계점: 0.647, 오탐률: 0.0045, 정탐률: 0.7857\n",
      "선택된 한계점: 0.647\n"
     ]
    }
   ],
   "source": [
    "'''1종 오류와 2종 오류 처리'''\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import scipy\n",
    "\n",
    "# 데이터셋을 가져오고, 원하는 오탐률(FPR)이 1%이하라는 것을 지정한다.\n",
    "X_train = scipy.sparse.load_npz(\"./datasets/training_data.npz\")\n",
    "Y_train = np.load(\"./datasets/training_labels.npy\")\n",
    "X_test = scipy.sparse.load_npz(\"./datasets/test_data.npz\")\n",
    "Y_test = np.load(\"./datasets/test_labels.npy\")\n",
    "desired_FPR = 0.01\n",
    "\n",
    "# 오탐률과 정탐률(TPR)을 계산하는 함수를 만든다.\n",
    "from sklearn.metrics import  confusion_matrix\n",
    "\n",
    "def FPR(Y_true, Y_pred):\n",
    "    # 오탐률을 계산한다.\n",
    "    CM = confusion_matrix(Y_true, Y_pred)\n",
    "    TN = CM[0][0]\n",
    "    FP = CM[0][1]\n",
    "    return FP / (FP + TN)\n",
    "\n",
    "def TPR(Y_true, Y_pred):\n",
    "    # 정탐률을 계산한다.\n",
    "    CM = confusion_matrix(Y_true, Y_pred)\n",
    "    TP = CM[1][1]\n",
    "    FN = CM[1][0]\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "# 한계점을 사용해 확률 벡터를 Bool 벡터로 변환하는 함수를 만든다.\n",
    "def perform_thresholding(vector, threshold):\n",
    "    # 벡터의 한계점\n",
    "    return [0 if x >= threshold else 1 for x in vector]\n",
    "\n",
    "# XGBoost 모델을 훈련하고, 훈련 데이터에 대한 예측 확률을 계산한다.\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, Y_train)\n",
    "clf_pred_prob = clf.predict_proba(X_train)\n",
    "\n",
    "# 예츨 확률 벡터를 확인한다.\n",
    "print(\"확률은 다음과 같다: \\n\")\n",
    "print(clf_pred_prob[0:5])\n",
    "\n",
    "# 1000개의 다른 한계점 값에 대해 반복하면서, 각 한계점에 대한 오탐률을 계산하고 FPR <= desiredFPR을 만족할 때의 한계점 값을 선택한다.\n",
    "M = 1000\n",
    "print(\"한계점 적합 중: \")\n",
    "for t in reversed(range(M)):\n",
    "    scaled_threshold = float(t) / M\n",
    "    threshold_prediction = perform_thresholding(clf_pred_prob[:,0], scaled_threshold)\n",
    "    FPR_ = FPR(Y_train, threshold_prediction)\n",
    "    TPR_ = TPR(Y_train, threshold_prediction)\n",
    "    print(\"{:03}번째 시도 한계점: {:.3f}, 오탐률: {:.4f}, 정탐률: {:.4f}\".format(t, scaled_threshold,FPR_,TPR_))\n",
    "    if FPR_ <= desired_FPR:\n",
    "        print(\"선택된 한계점: {}\".format(scaled_threshold))\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}